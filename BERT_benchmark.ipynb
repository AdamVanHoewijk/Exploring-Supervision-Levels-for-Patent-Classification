{"cells":[{"cell_type":"markdown","metadata":{"id":"mFwJ1A97dAza"},"source":["#DATASET\n","\n","https://huggingface.co/datasets/ccdv/patent-classification"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fBDuSBi2RvUg"},"outputs":[],"source":["%%capture\n","\n","import tensorflow as tf\n","import torch\n","\n","device = torch.device('cuda')\n","\n","!pip install transformers\n","!pip install datasets\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["folder_name = 'data_2'\n","num_classes = 14\n","\n","# path folder with models and data\n","PATH = '/content/drive/MyDrive/Masterthesis/MixText/data/' + folder_name + '/'"],"metadata":{"id":"FCLgYh45Ax9o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","import pandas as pd\n","\n","# create a tokenizer from patent bert\n","tokenizer = AutoTokenizer.from_pretrained('anferico/bert-for-patents', do_lower_case=True) # bert-base-uncased anferico/bert-for-patents\n","\n","# load the test data for the mixtext model\n","train_patents = pd.read_csv(PATH + 'train.csv', header=None)\n","train_patents = train_patents.dropna()\n","train_patents.head()"],"metadata":{"id":"BPbYEKtfCgSq"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HuNuAqF_deRk"},"outputs":[],"source":["import textwrap\n","import random\n","\n","wrapper = textwrap.TextWrapper(width=80)\n","\n","patent_examples = train_patents[2]\n","\n","for i in range(1):\n","  j = random.choice(patent_examples.index)\n","\n","  print('')\n","  print(wrapper.fill(patent_examples[j]))\n","  print('')"]},{"cell_type":"markdown","metadata":{"id":"vzDZuxL7c7TH"},"source":["# BERT MODEL"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WSA75UMddOW0"},"outputs":[],"source":["text = train_patents.iloc[200][2]\n","\n","tokens = tokenizer.tokenize(text)\n","\n","print(wrapper.fill(str(' '.join(tokens[0:512]))))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"twsZAX_recSH"},"outputs":[],"source":["input_ids = []\n","\n","lengths = []\n","\n","print('Tokenizing patents...')\n","\n","for sen in train_patents[2]:\n","  if ((len(input_ids) % 1000) == 0):\n","    print('  Read {:,} patents.'.format(len(input_ids)))\n","\n","  encoded_sent = tokenizer.encode(\n","      str(sen),\n","      add_special_tokens = True\n","  )\n","\n","  input_ids.append(encoded_sent)\n","\n","  lengths.append(len(encoded_sent))\n","\n","print('DONE.')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mhLDcZaYezQU"},"outputs":[],"source":["train_patents.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dX2HU2MDXPXC"},"outputs":[],"source":["from keras.utils import np_utils\n","\n","labels_num = train_patents[0].to_numpy().astype(int)\n","print(labels_num)\n","\n","labels = np_utils.to_categorical(labels_num)\n","print(labels)\n","print(labels.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q5twh4-QepBn"},"outputs":[],"source":["from keras.preprocessing.sequence import pad_sequences\n","\n","MAX_LEN = 256\n","\n","input_ids = pad_sequences(input_ids, maxlen = MAX_LEN, dtype='long', \n","                          value=0, truncating='post', padding='post')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yB5UXB-1fCI0"},"outputs":[],"source":["attention_masks = []\n","\n","for sent in input_ids:\n","  att_mask = [int(token_id > 0) for token_id in sent]\n","  \n","  attention_masks.append(att_mask)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eg0TXeJoZgPj"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids,\n","                                                                                    labels, random_state = 1, test_size=int(1))\n","\n","train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels, random_state=1, test_size=int(1))\n","\n","train_inputs.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lvNKgSLpZ6RP"},"outputs":[],"source":["train_inputs = torch.tensor(train_inputs)\n","validation_inputs = torch.tensor(validation_inputs)\n","\n","train_labels = torch.tensor(train_labels)\n","validation_labels = torch.tensor(validation_labels)\n","\n","train_masks = torch.tensor(train_masks)\n","validation_masks = torch.tensor(validation_masks)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r3kaFsxRa50r"},"outputs":[],"source":["from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","batch_size = 1\n","\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","validation_sampler = SequentialSampler(validation_data)\n","validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dtcBQZY1a8Mj"},"outputs":[],"source":["from transformers import BertForSequenceClassification, AdamW, BertConfig\n","\n","model = BertForSequenceClassification.from_pretrained(\n","    'anferico/bert-for-patents',\n","    num_labels = num_classes + 1,\n","    output_attentions = False,\n","    output_hidden_states = False\n",")\n","\n","model.cuda();"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jjeb4SGPa_Dk"},"outputs":[],"source":["optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5,\n","                  eps = 1e-8\n","                  )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nkhiRn8edFlE"},"outputs":[],"source":["from transformers import get_linear_schedule_with_warmup\n","\n","epochs = 5\n","\n","total_steps = len(train_dataloader) * epochs\n","\n","scheduler = get_linear_schedule_with_warmup(optimizer,\n","                                            num_warmup_steps = 0,\n","                                            num_training_steps = total_steps)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gs1ppvOpdH8L"},"outputs":[],"source":["import numpy as np\n","\n","def flat_accuracy(preds, labels):\n","  pred_flat = np.argmax(preds, axis=1).flatten()\n","  labels_flat = labels.flatten()\n","  return np.sum(pred_flat == labels_flat) / len(labels_flat)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1CjN5SRqdKTm"},"outputs":[],"source":["import time\n","import datetime\n","\n","def format_time(elapsed):\n","  elapsed_rounded = int(round((elapsed)))\n","  return str(datetime.timedelta(seconds=elapsed_rounded))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"scgCZ2phdO66"},"outputs":[],"source":["import random\n","import torch\n","\n","seed_val = 42\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","loss_values = []\n","\n","for epoch_i in range(0, epochs):\n","  t0 = time.time()\n","\n","  total_loss = 0\n","\n","  model.train()\n","\n","  for step, batch in enumerate(train_dataloader):\n","    if step % 100 == 0 and not step == 0:\n","      elapsed = format_time(time.time() - t0)\n","\n","      print(' Batch {:>5,} of {:>5,}. Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","    \n","    b_input_ids = batch[0].to(device)\n","    b_input_mask = batch[1].to(device)\n","    b_labels = batch[2].to(device)\n","\n","    model.zero_grad()\n","\n","    outputs = model(b_input_ids,\n","                    token_type_ids = None,\n","                    attention_mask = b_input_mask,\n","                    labels = b_labels\n","                    )\n","    \n","    loss = outputs[0]\n","\n","    total_loss += loss.item()\n","\n","    loss.backward()\n","\n","    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","    optimizer.step()\n","\n","    scheduler.step()\n","  \n","  avg_train_loss = total_loss / len(train_dataloader)\n","\n","  loss_values.append(avg_train_loss)\n","\n","  print('')\n","  print('Avg train loss: {0:2f}'.format(avg_train_loss))\n","\n","  print('')\n","  print('Runtime validation...')\n","\n","  t0 = time.time()\n","\n","  model.eval()\n","\n","  eval_loss, eval_accuracy = 0, 0\n","  nb_eval_steps, nb_eval_examples = 0, 0\n","\n","  for batch in validation_dataloader:\n","    batch = tuple(t.to(device) for t in batch)\n","\n","    b_input_ids, b_input_mask, b_labels = batch\n","\n","    with torch.no_grad():\n","      outputs = model(b_input_ids,\n","                      token_type_ids=None,\n","                      attention_mask=b_input_mask\n","                      )\n","    \n","    logits = outputs[0]\n","\n","    logits = logits.detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","\n","    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","\n","    eval_accuracy += tmp_eval_accuracy\n","\n","    nb_eval_steps += 1\n","\n","  print(' Accuracy: {0:2f}'.format(eval_accuracy/nb_eval_steps))\n","\n","print('')\n","print('Training complete')\n"]},{"cell_type":"code","source":["torch.save(model.state_dict(), PATH + 'bert_model.pt')"],"metadata":{"id":"CzvMwIXWZDcU"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_RkNxt6xs0oW"},"outputs":[],"source":["print(b_labels)"]},{"cell_type":"code","source":["test_patents = pd.read_csv(PATH + 'test.csv', header=None)\n","test_patents = test_patents.dropna()\n","test_patents.head()"],"metadata":{"id":"l7BO7GvhN-4p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sys import float_repr_style\n","test_input_ids = []\n","\n","for sen in test_patents[2]:\n","\n","  if ((len(input_ids) % 20000) == 0):\n","    print(' Read {:,} patents.'.format(len(input_ids)))\n","  \n","  encoded_sent = tokenizer.encode(\n","      str(sen),\n","      add_special_tokens = True,\n","      max_length = MAX_LEN,\n","  )\n","\n","  test_input_ids.append(encoded_sent)\n","\n","print('Done')\n","\n","test_labels_num = test_patents[0].to_numpy().astype(int)\n","test_labels = np_utils.to_categorical(test_labels_num)\n","\n","test_input_ids = pad_sequences(test_input_ids, maxlen=MAX_LEN,\n","                               dtype='long', truncating='post', padding='post')\n","\n","test_attention_masks = []\n","\n","for seq in test_input_ids:\n","  seq_mask = [float(i>0) for i in seq]\n","  test_attention_masks.append(seq_mask)\n","\n","test_inputs = torch.tensor(test_input_ids)\n","test_masks = torch.tensor(test_attention_masks)\n","test_labels = torch.tensor(test_labels)\n","\n","batch_size = 32\n","\n","test_data = TensorDataset(test_inputs, test_masks, test_labels)\n","test_sampler = SequentialSampler(test_data)\n","test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"],"metadata":{"id":"aq-ppMlXh9WF"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"liTzi4hmpi6c"},"outputs":[],"source":["print('Predicting labels for {:,} test patents...'.format(len(input_ids)))\n","\n","model.eval()\n","\n","predictions , true_labels = [], []\n","\n","t0 = time.time()\n","\n","for (step, batch) in enumerate(test_dataloader):\n","\n","  batch = tuple(t.to(device) for t in batch)\n","  \n","  if step % 100 == 0 and not step == 0:\n","      elapsed = format_time(time.time() - t0)\n","\n","      print(' Batch {:>5,} of {:>5,}. Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","\n","  b_input_ids, b_input_mask, b_labels = batch\n","  \n","\n","  with torch.no_grad():\n","      outputs = model(b_input_ids, token_type_ids=None, \n","                      attention_mask=b_input_mask)\n","\n","  logits = outputs[0]\n","\n","\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","  \n","  predictions.append(logits)\n","  true_labels.append(label_ids)\n","\n","print('    DONE.')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NUenSGIYprPk"},"outputs":[],"source":["predictions = np.concatenate(predictions, axis=0)\n","true_labels = np.concatenate(true_labels, axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dtU_M1A7ptqY"},"outputs":[],"source":["true_labels[0:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oMUQhsDBpx_F"},"outputs":[],"source":["predictions[0:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SC3WeDAu_Xso"},"outputs":[],"source":["pred = []\n","\n","for i in predictions:\n","  pred.append(np.argmax(i))\n","\n","true = []\n","\n","for i in true_labels:\n","  true.append(np.argmax(i))\n","\n","print(true)\n","print(pred)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rtzw4IexqF2d"},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n","from sklearn.metrics import f1_score\n","\n","print(round(f1_score(true, pred, average='macro')*100,1), \"/\", round(f1_score(true, pred, average='micro')*100,1), sep='')\n","\n","acc = accuracy_score(true, pred)\n","\n","print('Test Accuracy: %.3f' %acc)\n","print('F1 score macro: ', f1_score(true, pred, average='macro'))\n","print('F1 score micro: ', f1_score(true, pred, average='micro'))"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"BERT_benchmark.ipynb","provenance":[{"file_id":"195qtpM68HNmZ-aqMbCkgByzxbANd42NM","timestamp":1648112955357}],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}